{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc34b91-0fdc-49c4-a1a4-673e76e7513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to synthetic_ncRNA_dataset.csv\n",
      "Runtime: 28.03 seconds\n",
      "    ncRNA_1   ncRNA_2   ncRNA_3   ncRNA_4   ncRNA_5   ncRNA_6   ncRNA_7  \\\n",
      "0 -0.397403  0.852533  0.683079  0.942799 -1.631654 -1.388351 -1.623454   \n",
      "1 -1.670529  0.288005 -0.201638 -0.213920  0.888307 -1.039135  0.449562   \n",
      "2  0.195225 -0.872064 -0.918731  1.450675  0.529820 -1.152498 -1.284768   \n",
      "3 -2.083645  0.254206 -0.212502  0.999606  0.943054  1.885593  0.418506   \n",
      "4 -1.094652  1.331924  0.070002  1.231720 -0.276811  1.195791 -0.425053   \n",
      "\n",
      "    ncRNA_8   ncRNA_9  ncRNA_10  ...  ncRNA_92  ncRNA_93  ncRNA_94  ncRNA_95  \\\n",
      "0  0.903286  0.267438  0.708975  ...  1.403336  0.347992  0.268291  1.077486   \n",
      "1  1.367111 -1.658048 -1.520280  ...  1.479508  1.540875  0.607620 -0.141194   \n",
      "2  0.643163 -1.580155 -1.420649  ... -1.051406  1.185486 -0.964028 -0.790912   \n",
      "3 -0.454507  1.054717 -0.403555  ... -0.362057 -0.026868  1.102053 -0.679077   \n",
      "4 -1.930361  0.629722 -1.727435  ...  0.357038 -0.690740 -1.150822  0.129779   \n",
      "\n",
      "   ncRNA_96  ncRNA_97  ncRNA_98  ncRNA_99  ncRNA_100  label  \n",
      "0  0.058978 -0.099152 -0.264980 -1.704964  -1.179629      0  \n",
      "1 -0.570551  0.768647  1.603783  1.633369   0.716404      1  \n",
      "2  0.279638  0.975737 -0.796910  0.655943  -1.260541      1  \n",
      "3  1.189137  1.233919  0.355645  1.090850   0.834906      1  \n",
      "4 -0.423256  0.262136 -1.164355  2.038017   1.294554      1  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Start timing the script\n",
    "start_time = time.time()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of samples\n",
    "num_samples = 5000\n",
    "\n",
    "# Function to generate synthetic ncRNA data\n",
    "def generate_ncRNA_data(samples, features):\n",
    "    data = np.random.rand(samples, features)\n",
    "    noise = np.random.normal(0, 0.1, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# Function to generate binary labels\n",
    "def generate_labels(data, noise_level=0.1):\n",
    "    weights = np.random.rand(data.shape[1])\n",
    "    linear_combination = np.dot(data, weights) + np.random.randn(data.shape[0]) * noise_level\n",
    "    threshold = np.percentile(linear_combination, 50)\n",
    "    return (linear_combination > threshold).astype(int)\n",
    "\n",
    "# Generate synthetic data for ncRNAs\n",
    "ncRNA_data = generate_ncRNA_data(num_samples, 100)\n",
    "\n",
    "# Generate binary labels\n",
    "labels = generate_labels(ncRNA_data)\n",
    "\n",
    "# Create DataFrame with ncRNA features and labels\n",
    "columns = [f'ncRNA_{i}' for i in range(1, 101)]\n",
    "df = pd.DataFrame(ncRNA_data, columns=columns)\n",
    "df['label'] = labels\n",
    "\n",
    "# Function to standardize data\n",
    "def standardize_data(df, columns):\n",
    "    scaler = StandardScaler()\n",
    "    df[columns] = scaler.fit_transform(df[columns])\n",
    "    return df\n",
    "\n",
    "# Standardize ncRNA data\n",
    "df = standardize_data(df, columns)\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "output_file = 'synthetic_ncRNA_dataset.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# End timing the script\n",
    "end_time = time.time()\n",
    "\n",
    "# Print completion message and runtime\n",
    "print(f\"Dataset saved to {output_file}\")\n",
    "print(f\"Runtime: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ac70a9-364e-41cb-9f67-7b2ec91570c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nida\\AppData\\Local\\Temp\\ipykernel_6544\\2754287764.py:41: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  dnn_model = KerasClassifier(build_fn=create_dnn_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best parameters: {'optimizer': 'adam', 'neurons': 64, 'epochs': 50, 'dropout_rate': 0.4, 'batch_size': 16}\n",
      "32/32 [==============================] - 2s 9ms/step\n",
      " Accuracy: 96.20%\n",
      " Precision: 96.48%\n",
      " Recall: 96.10%\n",
      " F1 Score: 96.29%\n",
      " ROC AUC: 96.20%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6544\\2754287764.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Train Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the enhanced dataset\n",
    "df = pd.read_csv('synthetic_ncRNA_dataset.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model function\n",
    "def create_dnn_model(optimizer='adam', neurons=128, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons // 2, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons // 4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the KerasClassifier\n",
    "dnn_model = KerasClassifier(build_fn=create_dnn_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid with fewer options\n",
    "param_dist = {\n",
    "    'epochs': [50, 100],\n",
    "    'batch_size': [16, 32],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'neurons': [64, 128],\n",
    "    'dropout_rate': [0.3, 0.4]\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV with fewer iterations\n",
    "random_search = RandomizedSearchCV(estimator=dnn_model, param_distributions=param_dist, n_iter=5, cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(f'Best parameters: {random_search.best_params_}')\n",
    "\n",
    "# Train the best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_dnn = best_model.predict(X_test)\n",
    "# DRL Feature Transformation using Autoencoder\n",
    "# Evaluate the DNN model\n",
    "accuracy_dnn = accuracy_score(y_test, y_pred_dnn)\n",
    "precision_dnn = precision_score(y_test, y_pred_dnn)\n",
    "recall_dnn = recall_score(y_test, y_pred_dnn)\n",
    "f1_dnn = f1_score(y_test, y_pred_dnn)\n",
    "roc_auc_dnn = roc_auc_score(y_test, y_pred_dnn)\n",
    "\n",
    "print(f' Accuracy: {accuracy_dnn * 100:.2f}%')\n",
    "print(f' Precision: {precision_dnn * 100:.2f}%')\n",
    "print(f' Recall: {recall_dnn * 100:.2f}%')\n",
    "print(f' F1 Score: {f1_dnn * 100:.2f}%')\n",
    "print(f' ROC AUC: {roc_auc_dnn * 100:.2f}%')\n",
    "\n",
    "# Plot training history\n",
    "history = best_model.model.history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('DNN Training History')\n",
    "plt.show()\n",
    "\n",
    "# Visualization of performance metrics\n",
    "metrics_dnn = {\n",
    "    'Accuracy': accuracy_dnn,\n",
    "    'Precision': precision_dnn,\n",
    "    'Recall': recall_dnn,\n",
    "    'F1 Score': f1_dnn,\n",
    "    'ROC AUC': roc_auc_dnn\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(metrics_dnn.keys(), metrics_dnn.values())\n",
    "plt.title('DNN Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the confusion matrix\n",
    "cm_dnn = confusion_matrix(y_test, y_pred_dnn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_dnn, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('DNN Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acee06d-0942-412e-afe6-485d531c3f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
